{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 3: System Integration\n",
    "In this notebook, we connect our taxicab prediction workload to a monitoring system that leverages importance weighting. We:\n",
    "\n",
    "1. Walk through bare-bones architecture of the monitoring system built on top of `duckdb`\n",
    "2. Create a workload with simulated delays\n",
    "3. Plot metrics over time for our workload\n",
    "\n",
    "This notebook should be completed _after_ the second notebook (`2-importance-weighting√∏.ipynb`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date, datetime, timedelta\n",
    "from db import Task\n",
    "from pipeline import components\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import seaborn as sns\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PARAMETERS\n",
    "train_start_date = date(2020, 1, 1)\n",
    "train_end_date = date(2020, 1, 31)\n",
    "inference_start_date = date(2020, 2, 1)\n",
    "inference_end_date = date(2020, 3, 31)\n",
    "cadence = 7\n",
    "\n",
    "feature_columns = [\n",
    "    \"pickup_weekday\",\n",
    "    \"pickup_hour\",\n",
    "    \"pickup_minute\",\n",
    "    \"work_hours\",\n",
    "    \"passenger_count\",\n",
    "    \"trip_distance\",\n",
    "    \"trip_speed\",\n",
    "    \"PULocationID\",\n",
    "    \"DOLocationID\",\n",
    "    \"RatecodeID\",\n",
    "    \"congestion_surcharge\",\n",
    "    \"loc_code_diffs\",\n",
    "]\n",
    "label_column = \"high_tip_indicator\"\n",
    "\n",
    "WINDOW_SIZE = 60 * 60 * 24 * 7 # 7 days\n",
    "DELAY = 60 * 60 * 24 * 2 # 2 days\n",
    "ID_LEN = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monitoring System Architecture\n",
    "\n",
    "For our system prototype (built in Python and DuckDB), we have three main layers: interface, execution, and storage.\n",
    "\n",
    "### Interface\n",
    "\n",
    "The system has the following methods exposed to the client:\n",
    "\n",
    "* `log_prediction`\n",
    "* `log_feedback`\n",
    "* `register_metric`\n",
    "* `register_training_set`\n",
    "* `compute_metrics`\n",
    "\n",
    "Metrics consist of a name (e.g., accuracy), function that accepts `y_true` and `y_pred`, and a list of window sizes (seconds). When users want to retrieve the time-series metrics for each window, they can call `compute_metrics`. To handle importance weighting, `register_training_set` defines and precomputes a binning function.\n",
    "\n",
    "### Execution\n",
    "\n",
    "The two most complicated triggers are `log_prediction` and `log_feedback`. On `log_prediction`, the system applies the binning function to the features to compute a subgroup_id, increments a counter of unlabeled predictions for each window containing the prediction timestamp and subgroup_id, and writes the prediction to a predictions table. On `log_feedback`, the system identifies the corresponding prediction timestamp and subgroup_id, decrements relevant windows, and writes the feedback to a feedbacks table. Then on `compute_metrics`, we join predictions and feedbacks tables and compute metrics for each window, compute importance-weighted estimates for unlabeled predictions, and merge the joined result with the importance-weighted estimate.\n",
    "\n",
    "### Storage\n",
    "\n",
    "We maintain predictions, feedbacks, and metric tables. We also have a view of predictions join feedbacks. We maintain the counters corresponding to subgroup_ids in Python memory. We also keep the training set in-memory, but we can easily persist this to the DB.\n",
    "\n",
    "\n",
    "### Exercise\n",
    "\n",
    "We will create an instance of the `Task` class and register the accuracy metric.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_udf(labels, predictions):\n",
    "    \"\"\"\n",
    "    Returns the accuracy of the predictions.\n",
    "    \"\"\"\n",
    "    return accuracy_score(labels, np.round(predictions))\n",
    "\n",
    "def get_random_string(length):\n",
    "    # choose from all lowercase letter\n",
    "    letters = string.ascii_lowercase\n",
    "    result_str = \"\".join(random.choice(letters) for i in range(length))\n",
    "    return result_str\n",
    "\n",
    "task = Task(\"taxi_tip_prediction\")\n",
    "task.register_metric(\"accuracy\", accuracy_udf, window_sizes=[WINDOW_SIZE])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shreyashankar/miniforge3/envs/hawk/lib/python3.9/site-packages/sklearn/base.py:329: UserWarning: Trying to unpickle estimator DecisionTreeClassifier from version 0.24.2 when using version 1.0.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/modules/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/Users/shreyashankar/miniforge3/envs/hawk/lib/python3.9/site-packages/sklearn/base.py:329: UserWarning: Trying to unpickle estimator RandomForestClassifier from version 0.24.2 when using version 1.0.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/modules/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Register training set\n",
    "\n",
    "df = components.load_data(train_start_date, train_end_date)\n",
    "clean_df = components.clean_data(df, train_start_date, train_end_date)\n",
    "features_df = components.featurize_data(clean_df)\n",
    "train_predictions, _ = components.inference(features_df, feature_columns, label_column)\n",
    "task.register_training_set(\n",
    "    train_predictions,\n",
    "    feature_columns,\n",
    "    label_column,\n",
    "    \"prediction\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Workload with Simulated Delays\n",
    "\n",
    "We load all the inference data, clean, and run the model on it to generate predictions. Then we will create feedback / label delays by sampling from Exp(2 days)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shreyashankar/miniforge3/envs/hawk/lib/python3.9/site-packages/sklearn/base.py:329: UserWarning: Trying to unpickle estimator DecisionTreeClassifier from version 0.24.2 when using version 1.0.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/modules/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/Users/shreyashankar/miniforge3/envs/hawk/lib/python3.9/site-packages/sklearn/base.py:329: UserWarning: Trying to unpickle estimator RandomForestClassifier from version 0.24.2 when using version 1.0.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/modules/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "df = components.load_data(inference_start_date, inference_end_date)\n",
    "clean_df = components.clean_data(df, inference_start_date, inference_end_date)\n",
    "features_df = components.featurize_data(clean_df)\n",
    "inference_predictions, _ = components.inference(features_df, feature_columns, label_column)\n",
    "\n",
    "inference_predictions[feature_columns] = inference_predictions[feature_columns].astype(float)\n",
    "inference_predictions[\"features\"] = inference_predictions[feature_columns].apply(\n",
    "    lambda r: r.to_dict(), axis=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create delay column and dataframe of predictions and feedbacks\n",
    "\n",
    "pred_and_label_df = inference_predictions[\n",
    "        [\"features\", \"tpep_pickup_datetime\", \"prediction\", label_column]\n",
    "]\n",
    "pred_and_label_df = pred_and_label_df.rename(\n",
    "    columns={\n",
    "        \"tpep_pickup_datetime\": \"t_pred\",\n",
    "        \"prediction\": \"y_pred\",\n",
    "        label_column: \"y_true\",\n",
    "    }\n",
    ")\n",
    "pred_and_label_df = pred_and_label_df.assign(\n",
    "    delay=pd.to_timedelta(\n",
    "        np.random.exponential(scale=DELAY, size=len(pred_and_label_df)),\n",
    "        unit=\"s\",\n",
    "    ),\n",
    ")\n",
    "pred_and_label_df = pred_and_label_df.assign(\n",
    "    t_label=pred_and_label_df[\"t_pred\"] + pred_and_label_df[\"delay\"]\n",
    ")\n",
    "\n",
    "# Iterate through times and call log pred or label when necessary\n",
    "pred_and_label_df[\"identifier\"] = [\n",
    "    get_random_string(ID_LEN) for _ in range(len(pred_and_label_df))\n",
    "]\n",
    "pred_df = pred_and_label_df[\n",
    "    [\"t_pred\", \"y_pred\", \"identifier\", \"features\"]\n",
    "].reset_index(drop=True)\n",
    "label_df = pred_and_label_df[\n",
    "    [\"t_label\", \"y_true\", \"identifier\", \"features\"]\n",
    "].reset_index(drop=True)\n",
    "pred_df[\"type\"] = \"prediction\"\n",
    "label_df[\"type\"] = \"label\"\n",
    "pred_df.rename(columns={\"t_pred\": \"ts\", \"y_pred\": \"value\"}, inplace=True)\n",
    "label_df.rename(columns={\"t_label\": \"ts\", \"y_true\": \"value\"}, inplace=True)\n",
    "all_logs = pd.concat([pred_df, label_df]).reset_index(drop=True)\n",
    "all_logs.set_index(\"ts\", inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integrate `Task` with workload\n",
    "\n",
    "Here, we will iterate through `all_logs` (predictions + simulated delay feedbacks/labels) and log predictions and feedbacks to our monitoring system. We will compute metrics every day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing day: 2020-01-31 00:00:00\n",
      "There are 128766 predictions\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/shreyashankar/Documents/projects/oreilly-monitoring/3-monitoring-system-integration.ipynb Cell 12'\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/shreyashankar/Documents/projects/oreilly-monitoring/3-monitoring-system-integration.ipynb#ch0000015?line=8'>9</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mThere are \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(preds)\u001b[39m}\u001b[39;00m\u001b[39m predictions\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/shreyashankar/Documents/projects/oreilly-monitoring/3-monitoring-system-integration.ipynb#ch0000015?line=9'>10</a>\u001b[0m     \u001b[39mfor\u001b[39;00m t, \u001b[39mid\u001b[39m, pred, f \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(preds[\u001b[39m\"\u001b[39m\u001b[39mts\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mvalues, preds[\u001b[39m\"\u001b[39m\u001b[39midentifier\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mvalues, preds[\u001b[39m\"\u001b[39m\u001b[39mvalue\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mvalues, preds[\u001b[39m\"\u001b[39m\u001b[39mfeatures\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mvalues):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/shreyashankar/Documents/projects/oreilly-monitoring/3-monitoring-system-integration.ipynb#ch0000015?line=10'>11</a>\u001b[0m         task\u001b[39m.\u001b[39;49mlog_prediction(t, \u001b[39mid\u001b[39;49m, pred, f)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/shreyashankar/Documents/projects/oreilly-monitoring/3-monitoring-system-integration.ipynb#ch0000015?line=11'>12</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mPredictions logged for \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(preds)\u001b[39m}\u001b[39;00m\u001b[39m rows\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/shreyashankar/Documents/projects/oreilly-monitoring/3-monitoring-system-integration.ipynb#ch0000015?line=12'>13</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(feedbacks) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[0;32m~/Documents/projects/oreilly-monitoring/db/task.py:222\u001b[0m, in \u001b[0;36mTask.log_prediction\u001b[0;34m(self, ts, id, prediction, features)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcon\u001b[39m.\u001b[39mexecute(\n\u001b[1;32m    210\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mINSERT INTO predictions VALUES (?, ?, ?, ?, MAP(\u001b[39m\u001b[39m{\u001b[39;00mexecute_stmt\u001b[39m}\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m{\u001b[39;00mexecute_stmt\u001b[39m}\u001b[39;00m\u001b[39m))\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    211\u001b[0m     (\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    218\u001b[0m     ),\n\u001b[1;32m    219\u001b[0m )\n\u001b[1;32m    221\u001b[0m \u001b[39m# Increment window counter for subgroup\u001b[39;00m\n\u001b[0;32m--> 222\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mupdate_counters_only((subgroup_id, \u001b[39m1\u001b[39;49m), ts)\n",
      "File \u001b[0;32m~/Documents/projects/oreilly-monitoring/db/task.py:293\u001b[0m, in \u001b[0;36mTask.update_counters_only\u001b[0;34m(self, offset_subgroup, current_ts)\u001b[0m\n\u001b[1;32m    291\u001b[0m sk \u001b[39m=\u001b[39m SubgroupKey(offset_subgroup[\u001b[39m0\u001b[39m], t_pred, ws)\n\u001b[1;32m    292\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwindow_subgroup_counter[sk] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m offset_subgroup[\u001b[39m1\u001b[39m]\n\u001b[0;32m--> 293\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtainted_windows\u001b[39m.\u001b[39madd(WindowKey(t_pred, ws))\n",
      "File \u001b[0;32m<string>:1\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(_cls, window_start, window_size)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "metric_dfs = {}\n",
    "\n",
    "task.clear()\n",
    "for day, group_df in all_logs.groupby(pd.Grouper(freq=\"7D\")):\n",
    "    print(f\"Processing day: {day}\")\n",
    "    preds = group_df[group_df[\"type\"] == \"prediction\"].reset_index()\n",
    "    feedbacks = group_df[group_df[\"type\"] == \"label\"].reset_index()\n",
    "    if len(preds) > 0:\n",
    "        print(f\"There are {len(preds)} predictions\")\n",
    "        for t, id, pred, f in zip(preds[\"ts\"].values, preds[\"identifier\"].values, preds[\"value\"].values, preds[\"features\"].values):\n",
    "            task.log_prediction(t, id, pred, f)\n",
    "        print(f\"Predictions logged for {len(preds)} rows\")\n",
    "    if len(feedbacks) > 0:\n",
    "        print(f\"There are {len(feedbacks)} feedbacks\")\n",
    "        for t, id, feedback in zip(feedbacks[\"ts\"].values, feedbacks[\"identifier\"].values, feedbacks[\"value\"].values):\n",
    "            task.log_feedback(t, id, feedback)\n",
    "        print(f\"Feedbacks logged for {len(feedbacks)} rows\")\n",
    "    if len(preds) > 0 or len(feedbacks) > 0:\n",
    "        metrics = task.get_metrics()\n",
    "        metric_dfs[day] = (metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inefficiencies\n",
    "\n",
    "This is very slow! Mainly because we maintain O(# subgroups times # predictions made in 7 days) number of windows. Also, we don't log predictions/feedbacks in batch. How can we speed this up? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('hawk')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2a4a691971f6f80decb20cfeb22e20fe2e0bcf3f72c5df5bd5d3ca47468573df"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
